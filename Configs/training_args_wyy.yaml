adam_beta1: 0.9
adam_beta2: 0.95
adam_epsilon: 1e-8
callbacks:
- tensorboard
- early_stopping
data_limits:
  max_eval_samples: 100
  max_train_samples: null
dataloader_num_workers: 0
dataloader_pin_memory: false
ddp_find_unused_parameters: false
debug:
  debug_mode: false
  log_level: info
  log_on_each_node: true
early_stopping:
  enabled: false
  patience: 3
  threshold: 0.0
eval_steps: 200
evaluation:
  compute_metrics: true
  eval_accumulation_steps: null
  max_eval_samples: 100
evaluation_strategy: steps
fp16: true
gradient_accumulation_steps: 2
gradient_checkpointing: true
greater_is_better: false
hub:
  private: false
  repo_id: null
  token: null
learning_rate: 5.0e-05
load_best_model_at_end: true
logging_steps: 50
lora:
  loftq_config: null
  lora_alpha: 16
  lora_dropout: 0.1
  r: 8
  target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  use_lora: true
  use_rslora: false
lr_scheduler_type: cosine
max_eval_samples: 50
max_grad_norm: 1.0
max_steps: null
max_train_samples: 1500
metric_for_best_model: eval_loss
model_name_or_path: TinyLlama/TinyLlama_v1.1
num_train_epochs: 2
output_dir: models/tinyllama_ai_finetuned
overwrite_output_dir: true
per_device_eval_batch_size: 1
per_device_train_batch_size: 16
push_to_hub: false
qlora:
  bnb_4bit_compute_dtype: bfloat16
  bnb_4bit_quant_storage: uint8
  bnb_4bit_quant_type: nf4
  bnb_4bit_use_double_quant: true
  use_4bit: true
  use_qlora: false
report_to: tensorboard
save_only_model: false
save_safetensors: true
save_steps: 200
save_strategy: steps
save_total_limit: 3
seed: 42
tensorboard:
  log_dir: models/tinyllama_ai_finetuned/tensorboard
  update_freq: step
warmup_ratio: null
warmup_steps: 100
weight_decay: 0.0
