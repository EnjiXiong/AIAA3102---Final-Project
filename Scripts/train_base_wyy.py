#!/usr/bin/env python3
# scripts/train_base.py
"""
Enhanced Train script for tinyllama / causal LM using LoRA or QLoRA (4-bit).
æ”¯æŒæ›´å¤šè‡ªå®šä¹‰å‚æ•°å’Œè¯¦ç»†è®­ç»ƒç›‘æ§ã€‚
"""

import argparse
import os
import logging
from pathlib import Path
import json
import math
import random
from typing import Dict, List, Optional

import torch
import yaml
from datasets import load_dataset, Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    DataCollatorForLanguageModeling,
    TrainingArguments,
    Trainer,
    BitsAndBytesConfig,
    set_seed,
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from torch.utils.tensorboard import SummaryWriter
from transformers import TrainerCallback
import time
import numpy as np
import torch.nn.functional as F
import matplotlib.pyplot as plt
from tqdm.auto import tqdm


logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")

class EnhancedTensorBoardCallback(TrainerCallback):
    """
    å¢å¼ºçš„TensorBoardå›è°ƒï¼Œæ”¯æŒæ›´å¤šè®­ç»ƒæŒ‡æ ‡å’Œå®æ—¶è¿›åº¦æ˜¾ç¤º
    """
    
    def __init__(self, output_dir: str):
        super().__init__()
        self.output_dir = Path(output_dir)
        self.run_dir = self.output_dir / "tb_runs"
        self.run_dir.mkdir(parents=True, exist_ok=True)
        
        tb_subdir = time.strftime("%Y%m%d-%H%M%S")
        self.logdir = str(self.run_dir / tb_subdir)
        self.writer = SummaryWriter(log_dir=self.logdir)
        
        self.records = []
        self.last_log_step = None
        self.start_time = time.time()
        
        # è®­ç»ƒè¿›åº¦è·Ÿè¸ª
        self.progress_bar = None
        self.current_epoch = 0
        
    def on_train_begin(self, args, state, control, **kwargs):
        """è®­ç»ƒå¼€å§‹æ—¶åˆå§‹åŒ–è¿›åº¦æ¡"""
        total_steps = state.max_steps if state.max_steps else args.num_train_epochs * state.num_train_examples // args.train_batch_size
        self.progress_bar = tqdm(total=total_steps, desc="Training", unit="step")
        logger.info(f"ğŸš€ å¼€å§‹è®­ç»ƒï¼Œæ€»æ­¥æ•°: {total_steps}")
        
    def on_step_end(self, args, state, control, **kwargs):
        """æ¯ä¸€æ­¥ç»“æŸæ—¶æ›´æ–°è¿›åº¦æ¡"""
        if self.progress_bar:
            self.progress_bar.update(1)
            self.progress_bar.set_postfix({
                'loss': f"{state.log_history[-1].get('loss', 0):.4f}" if state.log_history else 'N/A',
                'epoch': state.epoch
            })
    
    def on_epoch_begin(self, args, state, control, **kwargs):
        """æ¯ä¸ªepochå¼€å§‹æ—¶æ›´æ–°"""
        self.current_epoch = state.epoch
        logger.info(f"ğŸ“… å¼€å§‹ç¬¬ {state.epoch:.1f} ä¸ªepoch")
        
    def _record_entry(self, step: int, entry: Dict):
        entry_with_step = {
            "step": int(step), 
            "timestamp": int(time.time()),
            "wall_time": time.time() - self.start_time
        }
        entry_with_step.update(entry)
        self.records.append(entry_with_step)

    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs is None:
            return
            
        step = int(state.global_step)
        
        # è®°å½•è®­ç»ƒæŒ‡æ ‡åˆ°TensorBoard
        metrics_to_log = {
            'loss': 'train/loss',
            'learning_rate': 'train/learning_rate', 
            'epoch': 'train/epoch',
            'grad_norm': 'train/grad_norm'
        }
        
        for log_key, tb_key in metrics_to_log.items():
            if log_key in logs:
                try:
                    value = logs[log_key]
                    if isinstance(value, list):
                        value = value[0]
                    self.writer.add_scalar(tb_key, float(value), step)
                except Exception as e:
                    logger.debug(f"Failed to log {log_key}: {e}")

        # è®°å½•åˆ°JSON
        entry = {}
        for key in metrics_to_log.keys():
            if key in logs:
                try:
                    entry[key] = float(logs[key]) if not isinstance(logs[key], list) else float(logs[key][0])
                except Exception:
                    entry[key] = logs[key]
                    
        if entry:
            self._record_entry(step, entry)
            self.last_log_step = step

    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        if metrics is None:
            return
            
        step = int(state.global_step)
        
        # è®°å½•è¯„ä¼°æŒ‡æ ‡
        for k, v in metrics.items():
            try:
                val = float(v)
                self.writer.add_scalar(f"eval/{k}", val, step)
            except Exception:
                continue
                
        # è®°å½•åˆ°JSON
        entry = {k: float(v) for k, v in metrics.items() if isinstance(v, (int, float))}
        if entry:
            self._record_entry(step, {"eval": entry})
            
        logger.info(f"ğŸ“Š è¯„ä¼°ç»“æœ (step {step}): {metrics}")

    def on_train_end(self, args, state, control, **kwargs):
        # å…³é—­è¿›åº¦æ¡
        if self.progress_bar:
            self.progress_bar.close()
            
        # ä¿å­˜è®­ç»ƒè®°å½•
        try:
            self.writer.flush()
            self.writer.close()
            
            # ä¿å­˜è¯¦ç»†çš„è®­ç»ƒæ—¥å¿—
            out_path = self.output_dir / "training_logs_detailed.json"
            with open(out_path, "w", encoding="utf-8") as f:
                json.dump(self.records, f, ensure_ascii=False, indent=2)
                
            # ç”Ÿæˆè®­ç»ƒæ›²çº¿å›¾
            self._plot_training_curves()
            
            logger.info(f"ğŸ“ˆ è®­ç»ƒå®Œæˆï¼æ—¥å¿—ä¿å­˜è‡³: {out_path}")
            logger.info(f"ğŸ“Š TensorBoardæ—¥å¿—åœ¨: {self.logdir}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜è®­ç»ƒæ—¥å¿—å¤±è´¥: {e}")

    def _plot_training_curves(self):
        """ç”Ÿæˆè®­ç»ƒæŸå¤±æ›²çº¿å›¾"""
        try:
            if not self.records:
                return
                
            # æå–è®­ç»ƒæŸå¤±
            train_steps = []
            train_losses = []
            
            for record in self.records:
                if 'loss' in record:
                    train_steps.append(record['step'])
                    train_losses.append(record['loss'])
            
            if train_steps:
                plt.figure(figsize=(10, 6))
                plt.plot(train_steps, train_losses, 'b-', alpha=0.7, label='Training Loss')
                plt.xlabel('Training Steps')
                plt.ylabel('Loss')
                plt.title('Training Loss Curve')
                plt.legend()
                plt.grid(True, alpha=0.3)
                
                # ä¿å­˜å›¾ç‰‡
                plot_path = self.output_dir / "training_loss_curve.png"
                plt.savefig(plot_path, dpi=300, bbox_inches='tight')
                plt.close()
                
                logger.info(f"ğŸ“¸ è®­ç»ƒæ›²çº¿å›¾ä¿å­˜è‡³: {plot_path}")
                
        except Exception as e:
            logger.warning(f"ç”Ÿæˆè®­ç»ƒæ›²çº¿å›¾å¤±è´¥: {e}")

def load_yaml(path: Path) -> Dict:
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)

def read_jsonl(path: Path) -> List[Dict]:
    objs = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            objs.append(json.loads(line))
    return objs

def make_dataset_from_jsonl(jsonl_path: Path, tokenizer, max_length: int, max_samples: Optional[int] = None):
    """
    åˆ›å»ºè®­ç»ƒæ•°æ®é›†ï¼Œæ”¯æŒæœ€å¤§æ ·æœ¬æ•°é™åˆ¶
    """
    from datasets import Dataset

    raw = read_jsonl(jsonl_path)
    
    # é™åˆ¶æ ·æœ¬æ•°é‡
    if max_samples and len(raw) > max_samples:
        logger.info(f"ğŸ“ é™åˆ¶æ•°æ®é›†ä» {len(raw)} åˆ° {max_samples} ä¸ªæ ·æœ¬")
        raw = raw[:max_samples]
    
    items = []
    for item in raw:
        p = item.get("prompt", "").strip()
        r = item.get("response", "").strip()
        items.append({"prompt": p, "response": r})

    # Tokenize
    all_prompts = [it["prompt"] for it in items]
    all_resps = [it["response"] for it in items]

    enc_prompts = tokenizer(all_prompts, add_special_tokens=False)["input_ids"]
    enc_resps = tokenizer(all_resps, add_special_tokens=False)["input_ids"]

    input_ids_list = []
    labels_list = []
    attention_masks = []

    for p_ids, r_ids in zip(enc_prompts, enc_resps):
        full = p_ids + r_ids

        if len(full) > max_length:
            full = full[-max_length:]
            if len(r_ids) >= max_length:
                resp_start = 0
            else:
                resp_start = max(0, len(full) - len(r_ids))
        else:
            resp_start = len(p_ids)

        labels = [-100] * len(full)
        for i in range(resp_start, len(full)):
            labels[i] = full[i]

        pad_len = max_length - len(full)
        input_ids = full + [tokenizer.pad_token_id] * pad_len
        labels = labels + [-100] * pad_len
        attention_mask = [1] * len(full) + [0] * pad_len

        input_ids_list.append(input_ids)
        labels_list.append(labels)
        attention_masks.append(attention_mask)

    ds = Dataset.from_dict({
        "input_ids": input_ids_list,
        "labels": labels_list,
        "attention_mask": attention_masks
    })
    ds = ds.with_format(type="torch")
    
    logger.info(f"ğŸ“Š æ•°æ®é›†åˆ›å»ºå®Œæˆ: {len(ds)} ä¸ªæ ·æœ¬")
    return ds

def compute_metrics(eval_pred):
    """
    è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    """
    try:
        predictions, labels = eval_pred
        if predictions is None:
            return {}
            
        preds = np.array(predictions)
        lab = np.array(labels)

        if preds.ndim != 3:
            return {}

        logits = torch.from_numpy(preds)
        labels_t = torch.from_numpy(lab).long()

        log_probs = F.log_softmax(logits, dim=-1)
        B, S = labels_t.shape
        mask = labels_t != -100
        
        if mask.sum().item() == 0:
            return {}

        labels_exp = labels_t.unsqueeze(-1)
        token_log_probs = torch.gather(log_probs, dim=-1, index=labels_exp).squeeze(-1)
        nll = -token_log_probs[mask]
        token_loss = nll.mean().item()
        perplexity = float(torch.exp(torch.tensor(token_loss)).item())

        return {"token_loss": float(token_loss), "perplexity": perplexity}
    except Exception as e:
        return {}

def main():
    parser = argparse.ArgumentParser(description="å¢å¼ºçš„LLMå¾®è°ƒè„šæœ¬")
    
    # åŸºç¡€å‚æ•°
    parser.add_argument("--config_dir", type=str, default="configs", help="é…ç½®æ–‡ä»¶ç›®å½•")
    parser.add_argument("--train_file", type=str, required=True, help="è®­ç»ƒæ•°æ®æ–‡ä»¶")
    parser.add_argument("--valid_file", type=str, required=True, help="éªŒè¯æ•°æ®æ–‡ä»¶")
    parser.add_argument("--overwrite_output_dir", action="store_true", help="è¦†ç›–è¾“å‡ºç›®å½•")
    parser.add_argument("--push_to_hub", action="store_true", help="æ¨é€åˆ°HuggingFace Hub")
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--num_train_epochs", type=int, default=None, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--learning_rate", type=float, default=None, help="å­¦ä¹ ç‡")
    parser.add_argument("--per_device_train_batch_size", type=int, default=None, help="è®­ç»ƒæ‰¹æ¬¡å¤§å°")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=None, help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°")
    parser.add_argument("--max_steps", type=int, default=None, help="æœ€å¤§è®­ç»ƒæ­¥æ•°")
    parser.add_argument("--metric_for_best_model", type=str, default=None, help="æœ€ä½³æ¨¡å‹æŒ‡æ ‡")
    
    # æ–°å¢è‡ªå®šä¹‰å‚æ•°
    parser.add_argument("--max_eval_samples", type=int, default=None, help="æœ€å¤§è¯„ä¼°æ ·æœ¬æ•°")
    parser.add_argument("--max_train_samples", type=int, default=None, help="æœ€å¤§è®­ç»ƒæ ·æœ¬æ•°")
    parser.add_argument("--use_lora", type=bool, default=None, help="æ˜¯å¦ä½¿ç”¨LoRA")
    parser.add_argument("--use_qlora", type=bool, default=None, help="æ˜¯å¦ä½¿ç”¨QLoRA")
    parser.add_argument("--lora_rank", type=int, default=None, help="LoRAç§©")
    parser.add_argument("--lora_alpha", type=int, default=None, help="LoRA alphaå‚æ•°")
    parser.add_argument("--lora_dropout", type=float, default=None, help="LoRA dropout")
    parser.add_argument("--seed", type=int, default=None, help="éšæœºç§å­")
    
    args = parser.parse_args()

    # åŠ è½½é…ç½®
    cfg_dir = Path(args.config_dir)
    training_cfg = load_yaml(cfg_dir / "training_args.yaml")
    model_cfg = load_yaml(cfg_dir / "model_config.yaml")
    eval_cfg = load_yaml(cfg_dir / "eval_config.yaml")

    # ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®
    config_overrides = {
        'num_train_epochs': args.num_train_epochs,
        'learning_rate': args.learning_rate,
        'per_device_train_batch_size': args.per_device_train_batch_size,
        'gradient_accumulation_steps': args.gradient_accumulation_steps,
        'max_steps': args.max_steps,
        'metric_for_best_model': args.metric_for_best_model,
        'seed': args.seed,
    }
    
    for key, value in config_overrides.items():
        if value is not None:
            training_cfg[key] = value

    # LoRAå‚æ•°è¦†ç›–
    if args.use_lora is not None:
        training_cfg["lora"]["use_lora"] = args.use_lora
    if args.use_qlora is not None:
        training_cfg["qlora"]["use_qlora"] = args.use_qlora
    if args.lora_rank is not None:
        training_cfg["lora"]["r"] = args.lora_rank
    if args.lora_alpha is not None:
        training_cfg["lora"]["lora_alpha"] = args.lora_alpha
    if args.lora_dropout is not None:
        training_cfg["lora"]["lora_dropout"] = args.lora_dropout

    # è®¾ç½®éšæœºç§å­
    seed = training_cfg.get("seed", 42)
    set_seed(seed)
    random.seed(seed)
    logger.info(f"ğŸ² è®¾ç½®éšæœºç§å­: {seed}")

    device = "cuda" if torch.cuda.is_available() else "cpu"
    logger.info(f"ğŸ–¥ï¸  è¿è¡Œè®¾å¤‡: {device}")

    # åŠ è½½æ¨¡å‹å’Œtokenizer
    model_name_or_path = training_cfg.get("model_name_or_path", model_cfg.get("model_name_or_path"))
    if model_name_or_path is None:
        raise ValueError("âŒ æœªåœ¨é…ç½®ä¸­æŒ‡å®šæ¨¡å‹åç§°/è·¯å¾„")

    tokenizer = AutoTokenizer.from_pretrained(model_cfg.get("tokenizer_name_or_path", model_name_or_path))
    if tokenizer.pad_token is None:
        logger.info("ğŸ”§ Tokenizeræ²¡æœ‰pad_tokenï¼Œè®¾ç½®pad_token = eos_token")
        tokenizer.pad_token = tokenizer.eos_token

    # é‡åŒ–é…ç½®
    use_qlora = training_cfg.get("qlora", {}).get("use_qlora", False)
    use_4bit = training_cfg.get("qlora", {}).get("use_4bit", False) and use_qlora

    bnb_config = None
    if use_4bit:
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type=training_cfg["qlora"].get("bnb_4bit_quant_type", "nf4"),
            bnb_4bit_use_double_quant=training_cfg["qlora"].get("bnb_4bit_use_double_quant", True),
            bnb_4bit_compute_dtype=getattr(torch, training_cfg["qlora"].get("bnb_4bit_compute_dtype", "bfloat16")),
        )
        logger.info(f"ğŸ”§ ä½¿ç”¨4-bit QLoRAé…ç½®")

    # åŠ è½½æ¨¡å‹
    try:
        if bnb_config is not None:
            model = AutoModelForCausalLM.from_pretrained(
                model_name_or_path,
                quantization_config=bnb_config,
                device_map="auto",
                trust_remote_code=True,
            )
        else:
            model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map="auto")
    except Exception as e:
        logger.warning(f"âŒ ä½¿ç”¨device_map=autoåŠ è½½å¤±è´¥ï¼Œå°è¯•CPUåŠ è½½: {e}")
        model = AutoModelForCausalLM.from_pretrained(model_name_or_path, low_cpu_mem_usage=True)
        model.to(device)

    # QLoRAå‡†å¤‡
    if use_qlora and bnb_config is not None:
        logger.info("ğŸ”§ å‡†å¤‡k-bitè®­ç»ƒ (QLoRA)")
        model = prepare_model_for_kbit_training(model)

    # LoRAé…ç½®
    use_lora = training_cfg.get("lora", {}).get("use_lora", True)
    if use_lora:
        lora_cfg = training_cfg.get("lora", {})
        lora_config = LoraConfig(
            r=lora_cfg.get("r", 8),
            lora_alpha=lora_cfg.get("lora_alpha", 32),
            target_modules=lora_cfg.get("target_modules", ["q_proj", "k_proj", "v_proj", "o_proj"]),
            lora_dropout=lora_cfg.get("lora_dropout", 0.1),
            bias="none",
            task_type="CAUSAL_LM",
        )
        model = get_peft_model(model, lora_config)
        logger.info(f"ğŸ¯ LoRAé€‚é…å™¨å·²é™„åŠ åˆ°æ¨¡å‹ (rank={lora_config.r}, alpha={lora_config.lora_alpha})")

    # å‡†å¤‡æ•°æ®é›†
    max_input_length = model_cfg.get("max_input_length", 512)
    
    # ä½¿ç”¨è‡ªå®šä¹‰çš„æœ€å¤§æ ·æœ¬æ•°
    max_train_samples = args.max_train_samples
    max_eval_samples = args.max_eval_samples or 50  # é»˜è®¤50ä¸ªè¯„ä¼°æ ·æœ¬
    
    train_ds = make_dataset_from_jsonl(
        Path(args.train_file), 
        tokenizer, 
        max_input_length, 
        max_samples=max_train_samples
    )
    valid_ds = make_dataset_from_jsonl(
        Path(args.valid_file), 
        tokenizer, 
        max_input_length, 
        max_samples=max_eval_samples
    )
    
    logger.info(f"ğŸ“Š è®­ç»ƒé›†: {len(train_ds)} ä¸ªæ ·æœ¬")
    logger.info(f"ğŸ“Š éªŒè¯é›†: {len(valid_ds)} ä¸ªæ ·æœ¬")

    # æ•°æ®æ•´ç†å™¨
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer, 
        mlm=False, 
        pad_to_multiple_of=8
    )

    # è®­ç»ƒå‚æ•°
    output_dir = training_cfg.get("output_dir", "models/finetuned_model")
    
    # ä¿®å¤è®­ç»ƒå‚æ•°
    max_steps = training_cfg.get("max_steps", -1)
    if max_steps is None:
        max_steps = -1

    # è®¾ç½®æœ€ä½³æ¨¡å‹æŒ‡æ ‡
    metric_for_best_model = training_cfg.get("metric_for_best_model", "eval_loss")
    greater_is_better = training_cfg.get("greater_is_better", False)
    if metric_for_best_model == "eval_loss":
        greater_is_better = False
    elif metric_for_best_model in ["accuracy", "f1", "perplexity"]:
        greater_is_better = True

    training_args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=training_cfg.get("per_device_train_batch_size", 2),
        per_device_eval_batch_size=training_cfg.get("per_device_eval_batch_size", 4),
        gradient_accumulation_steps=training_cfg.get("gradient_accumulation_steps", 1),
        num_train_epochs=training_cfg.get("num_train_epochs", 2),
        max_steps=max_steps,
        learning_rate=training_cfg.get("learning_rate", 5e-5),
        weight_decay=training_cfg.get("weight_decay", 0.0),
        logging_steps=training_cfg.get("logging_steps", 50),
        eval_strategy=training_cfg.get("evaluation_strategy", "steps"),
        eval_steps=training_cfg.get("eval_steps", 200),
        save_steps=training_cfg.get("save_steps", 200),
        save_total_limit=training_cfg.get("save_total_limit", 3),
        fp16=training_cfg.get("fp16", True),
        gradient_checkpointing=training_cfg.get("gradient_checkpointing", True),
        load_best_model_at_end=training_cfg.get("load_best_model_at_end", True),
        metric_for_best_model=metric_for_best_model,
        greater_is_better=greater_is_better,
        push_to_hub=training_cfg.get("push_to_hub", False) or args.push_to_hub,
        report_to=training_cfg.get("report_to", "none"),
        remove_unused_columns=False,
        overwrite_output_dir=args.overwrite_output_dir or training_cfg.get("overwrite_output_dir", False),
        logging_dir=str(Path(output_dir) / "tensorboard"),
        # æ–°å¢å‚æ•°ä»¥æ”¹å–„è®­ç»ƒä½“éªŒ
        dataloader_pin_memory=False,
        dataloader_num_workers=0,
        warmup_steps=training_cfg.get("warmup_steps", 100),
        lr_scheduler_type=training_cfg.get("lr_scheduler_type", "cosine"),
    )

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_ds,
        eval_dataset=valid_ds,
        data_collator=data_collator,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
        callbacks=[EnhancedTensorBoardCallback(output_dir=output_dir)],
    )

    # æ‰“å°è®­ç»ƒä¿¡æ¯
    logger.info("ğŸš€ å¼€å§‹è®­ç»ƒ")
    logger.info(f"  ğŸ“ è®­ç»ƒæ ·æœ¬æ•°: {len(train_ds)}")
    logger.info(f"  ğŸ“ éªŒè¯æ ·æœ¬æ•°: {len(valid_ds)}")
    logger.info(f"  ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    logger.info(f"  ğŸ“Š æœ€ä½³æ¨¡å‹æŒ‡æ ‡: {metric_for_best_model}")
    logger.info(f"  ğŸ”§ LoRA: {use_lora}")
    logger.info(f"  ğŸ”§ QLoRA: {use_qlora}")
    logger.info(f"  âš™ï¸  å­¦ä¹ ç‡: {training_args.learning_rate}")
    logger.info(f"  âš™ï¸  æ‰¹æ¬¡å¤§å°: {training_args.per_device_train_batch_size}")
    logger.info(f"  âš™ï¸  æ¢¯åº¦ç´¯ç§¯: {training_args.gradient_accumulation_steps}")

    # å¼€å§‹è®­ç»ƒ
    start_time = time.time()
    trainer.train()
    training_time = time.time() - start_time
    
    logger.info(f"âœ… è®­ç»ƒå®Œæˆï¼è€—æ—¶: {training_time/60:.2f} åˆ†é’Ÿ")

    # ä¿å­˜æ¨¡å‹
    logger.info("ğŸ’¾ ä¿å­˜æ¨¡å‹å’Œtokenizer...")
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    
    # ä¿å­˜è®­ç»ƒé…ç½®
    config_save_path = Path(output_dir) / "training_config.json"
    with open(config_save_path, 'w', encoding='utf-8') as f:
        json.dump({
            'training_cfg': training_cfg,
            'model_cfg': model_cfg,
            'eval_cfg': eval_cfg,
            'training_time_minutes': training_time/60,
            'final_metrics': trainer.state.log_history[-1] if trainer.state.log_history else {}
        }, f, indent=2, ensure_ascii=False)
    
    logger.info(f"ğŸ’¾ æ¨¡å‹å’Œé…ç½®ä¿å­˜åˆ°: {output_dir}")

    # å¯é€‰ï¼šæ¨é€åˆ°Hub
    if training_args.push_to_hub:
        try:
            logger.info("ğŸŒ æ¨é€æ¨¡å‹åˆ°HuggingFace Hub...")
            trainer.push_to_hub()
            logger.info("âœ… æ¨é€å®Œæˆ")
        except Exception as e:
            logger.warning(f"âŒ æ¨é€åˆ°Hubå¤±è´¥: {e}")

    logger.info("ğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")


if __name__ == "__main__":
    main()
