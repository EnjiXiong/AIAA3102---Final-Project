{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1_TNwkGn1dSjWdJy4G1ungwT_2KR2f7e7","authorship_tag":"ABX9TyOpfWsBusotw+QixI9EBhuM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"c10afdf178e34444b2ee402c0b23e422":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_75fbcb56d4b3411483568e3b7b8d3aad","IPY_MODEL_6a5eb438562f48eb8599a75566950eee","IPY_MODEL_08af865234ca475fb55f1cbcd84534f2"],"layout":"IPY_MODEL_cd6e071fee1349889e8a9514171bcf22"}},"75fbcb56d4b3411483568e3b7b8d3aad":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_016d202a7eca43a4b61743011487f3a0","placeholder":"​","style":"IPY_MODEL_a603354272b74921ad5a20027240c9fa","value":"README.md: "}},"6a5eb438562f48eb8599a75566950eee":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed16937f3a794bb6a3b1edd57d80322f","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_23129249835e4a1dbca43caaf25e3a68","value":1}},"08af865234ca475fb55f1cbcd84534f2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_48f9f237ed494c9683046547a997827b","placeholder":"​","style":"IPY_MODEL_ba675e0f023646058c3178064d1aab7f","value":" 3.84k/? [00:00&lt;00:00, 265kB/s]"}},"cd6e071fee1349889e8a9514171bcf22":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"016d202a7eca43a4b61743011487f3a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a603354272b74921ad5a20027240c9fa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ed16937f3a794bb6a3b1edd57d80322f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"23129249835e4a1dbca43caaf25e3a68":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"48f9f237ed494c9683046547a997827b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba675e0f023646058c3178064d1aab7f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9936b94b8b6c41caa0c9f146c30feec5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2c1d9ff368ba4839a491db4af97bdd51","IPY_MODEL_5c31b9f5ff0543c28739f2ca275c9a9c","IPY_MODEL_353d69fd119e4a2aad5ca229006444a1"],"layout":"IPY_MODEL_60b42b8ae946401f9f427d5e458b1e9e"}},"2c1d9ff368ba4839a491db4af97bdd51":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e270a72912c434dab1e44b59b70f17c","placeholder":"​","style":"IPY_MODEL_62e9caceacf14ace8bc7eb28b2c24e22","value":"combined_dataset.json: "}},"5c31b9f5ff0543c28739f2ca275c9a9c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2c0b5672db3740558e2962110dcd7b50","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_043f5763ae5e4f3a83b287604e0204ec","value":1}},"353d69fd119e4a2aad5ca229006444a1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_02e18e07845c485e9573c93c047b0abc","placeholder":"​","style":"IPY_MODEL_3bcba4f9921a4667b885f60a5d9739a5","value":" 4.79M/? [00:00&lt;00:00, 9.63MB/s]"}},"60b42b8ae946401f9f427d5e458b1e9e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e270a72912c434dab1e44b59b70f17c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62e9caceacf14ace8bc7eb28b2c24e22":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2c0b5672db3740558e2962110dcd7b50":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"043f5763ae5e4f3a83b287604e0204ec":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"02e18e07845c485e9573c93c047b0abc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3bcba4f9921a4667b885f60a5d9739a5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"51226f6f18d04ee2950e89d5cc304adb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_26af474b96334a1e90cadd8a5e7516ae","IPY_MODEL_c65ced7fe14f4483be84ea50f8049677","IPY_MODEL_577087110d73443ebfbdca4c65c0a8e3"],"layout":"IPY_MODEL_b264bfcd4d784d39ad7e1e53ad8e8abd"}},"26af474b96334a1e90cadd8a5e7516ae":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7c1f5188048c45fabf02e8e98b2dad65","placeholder":"​","style":"IPY_MODEL_14e460ae33a34339adcada826341f058","value":"Generating train split: 100%"}},"c65ced7fe14f4483be84ea50f8049677":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ffa1a55ec4d24a03805bf9f1e7fbb647","max":3512,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cf43655276084c189402ef400c795bfa","value":3512}},"577087110d73443ebfbdca4c65c0a8e3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_53ac9eb665ba49f9a597b728fef8d557","placeholder":"​","style":"IPY_MODEL_92b29a4b918d472cb42608e786541e56","value":" 3512/3512 [00:00&lt;00:00, 48744.03 examples/s]"}},"b264bfcd4d784d39ad7e1e53ad8e8abd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c1f5188048c45fabf02e8e98b2dad65":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"14e460ae33a34339adcada826341f058":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ffa1a55ec4d24a03805bf9f1e7fbb647":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf43655276084c189402ef400c795bfa":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"53ac9eb665ba49f9a597b728fef8d557":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92b29a4b918d472cb42608e786541e56":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":[],"metadata":{"id":"LRLI6TzAWvGf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 生成yaml文件"],"metadata":{"id":"z8mGTZOCVVRo"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"joFapO0mUJ1H","executionInfo":{"status":"ok","timestamp":1762686899964,"user_tz":-480,"elapsed":42,"user":{"displayName":"Andy","userId":"06581432161554596484"}},"outputId":"0d3d108a-d740-4ed0-bced-b558e1fde8b2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing /content/drive/MyDrive/AIAA3102/Final_Project/Configs/training_args.yaml\n"]}],"source":["%%writefile /content/drive/MyDrive/AIAA3102/Final_Project/Configs/training_args.yaml\n","# training_args.yaml\n","# 训练相关参数（适配 Colab T4 + QLoRA / LoRA）\n","seed: 42\n","\n","# 模型与输出\n","model_name_or_path: \"TinyLlama/TinyLlama_v1.1\"  # 替换为你使用的 tinyllama repo\n","output_dir: \"/content/drive/MyDrive/AIAA3102/Final_Project/Models/tinyllama_ai_finetuned\"\n","\n","# LoRA / PEFT 设置（若使用 LoRA）\n","lora:\n","  use_lora: true\n","  r: 8\n","  lora_alpha: 32\n","  lora_dropout: 0.1\n","  target_modules: [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]   # 依据 tinyllama 实际模块名调整\n","\n","# QLoRA（4-bit）设置（如使用 QLoRA，请设置 use_qlora: true）\n","qlora:\n","  use_qlora: true\n","  use_4bit: true\n","  bnb_4bit_quant_type: \"nf4\"        # nf4 推荐，亦可选 \"fp4\"\n","  bnb_4bit_compute_dtype: \"bfloat16\" # 若不支持可设为 \"float16\"\n","  bnb_4bit_use_double_quant: true\n","\n","# 数据/训练超参\n","per_device_train_batch_size: 2\n","per_device_eval_batch_size: 4\n","gradient_accumulation_steps: 1\n","num_train_epochs: 2\n","max_steps: null            # 若指定 steps，则覆盖 num_train_epochs\n","save_steps: 200\n","save_total_limit: 3\n","\n","# 学习率 / 优化\n","learning_rate: 5e-5\n","weight_decay: 0.0\n","adam_beta1: 0.9\n","adam_beta2: 0.95\n","adam_epsilon: 1e-8\n","\n","# 日志与评估\n","logging_steps: 50\n","evaluation_strategy: \"steps\"   # \"no\" | \"steps\" | \"epoch\"\n","eval_steps: 200\n","load_best_model_at_end: true\n","metric_for_best_model: \"refusal_rate\"  # 可自定义 (accuracy / loss / refusal_rate)\n","\n","# 设备与数值\n","fp16: true\n","gradient_checkpointing: true\n","\n","# 其它\n","push_to_hub: false           # 若训练结束后自动 push，可设为 true\n","report_to: \"none\"           # \"wandb\" / \"tensorboard\" / \"none\"\n","overwrite_output_dir: true\n"]},{"cell_type":"code","source":["%%writefile /content/drive/MyDrive/AIAA3102/Final_Project/Configs/model_config.yaml\n","# model_config.yaml\n","# 模型 / tokenizer 层级设置与输入格式（用于数据处理与推理）\n","model_name_or_path: \"TinyLlama/TinyLlama_v1.1\"\n","tokenizer_name_or_path: \"TinyLlama/TinyLlama_v1.1\"\n","pad_token: \"</s>\"\n","eos_token: \"</s>\"\n","bos_token: \"<s>\"\n","max_input_length: 512\n","max_generation_length: 128\n","\n","# 训练输入格式（causal lm）\n","# prompt_format 用于把 prompt 与 reference 拼接成训练文本（causal LM）\n","# 使用占位符 {prompt} 和 {response}\n","prompt_format: |\n","  ### 问：\n","  {prompt}\n","  ### 答：\n","  {response}\n","\n","# 对齐 labels 设置（若需要 special tokens）\n","label_smoothing: 0.0\n","\n","# tinyllama 特定层名称（用于 LoRA target modules）\n","# 如果不确定，可在脚本中打印 model.named_modules() 确认\n","gpt_attn_module_names:\n","  - \"q_proj\"\n","  - \"k_proj\"\n","  - \"v_proj\"\n","  - \"o_proj\"\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kRp-XzKdU9Ez","executionInfo":{"status":"ok","timestamp":1762689501278,"user_tz":-480,"elapsed":90,"user":{"displayName":"Andy","userId":"06581432161554596484"}},"outputId":"736d3bf9-4667-4232-edff-3a90a8325e83"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/drive/MyDrive/AIAA3102/Final_Project/Configs/model_config.yaml\n"]}]},{"cell_type":"code","source":["%%writefile /content/drive/MyDrive/AIAA3102/Final_Project/Configs/eval_config.yaml\n","# eval_config.yaml\n","# 评估/推理相关配置（生成策略、拒答判定、指标）\n","seed: 42\n","\n","# 生成参数\n","generation:\n","  do_sample: false\n","  num_beams: 1\n","  max_new_tokens: 128\n","  temperature: 0.0\n","  top_k: 50\n","  top_p: 0.95\n","  repetition_penalty: 1.0\n","  eos_token_id: null   # 若需要，可从 tokenizer 获取并在脚本里填充\n","\n","# 拒答判定（关键词匹配 + 概率阈值可合并使用）\n","refusal_detection:\n","  # 若生成文本包含任一关键词则判为拒答\n","  refusal_keywords:\n","    - \"我不知道\"\n","    - \"抱歉，我无法\"\n","    - \"无法确定\"\n","    - \"I don't know\"\n","    - \"I am not sure\"\n","  # 可选：基于生成概率/score 的阈值判定（需在脚本中实现）\n","  use_score_threshold: false\n","  score_threshold: 0.0\n","\n","# 评估指标（会在 eval 脚本中计算）\n","metrics:\n","  - \"refusal_rate\"      # 对 unknown 测试集：模型拒答的比例\n","  - \"unknown_false_positive_rate\"  # 模型对 unknown 生成答案（误答率）\n","  - \"known_exact_match\" # 对 known 样本的 EM（或可替换为 rouge/f1）\n","  - \"known_rouge_l\"\n","\n","# 评估数据路径（相对于项目根目录）\n","datasets:\n","  known_eval: \"data/valid.jsonl\"\n","  unknown_eval: \"data/unknown_test.jsonl\"\n","\n","# 评价器（可选：使用 LLM-as-judge）\n","judge:\n","  use_llm_judge: false\n","  judge_model: \"gpt-4o-mini\"   # 若环境可调，用于主观质量评分（需要 API）\n","  judge_prompts_path: \"configs/judge_prompts.yaml\"\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q7Z7FcUmVJs9","executionInfo":{"status":"ok","timestamp":1762687000951,"user_tz":-480,"elapsed":60,"user":{"displayName":"Andy","userId":"06581432161554596484"}},"outputId":"d5a05129-6ab9-4366-b7ab-e736c73fa55b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing /content/drive/MyDrive/AIAA3102/Final_Project/Configs/eval_config.yaml\n"]}]},{"cell_type":"markdown","source":["# 生成py脚本文件"],"metadata":{"id":"SNV_g_41VdUW"}},{"cell_type":"code","source":["%%writefile /content/drive/MyDrive/AIAA3102/Final_Project/Scripts/preprocess.py"],"metadata":{"id":"6D4AATt_WG3c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%writefile /content/drive/MyDrive/AIAA3102/Final_Project/Scripts/train_base.py\n","#!/usr/bin/env python3\n","# scripts/train_base.py\n","\"\"\"\n","Train script for tinyllama / causal LM using LoRA or QLoRA (4-bit).\n","Reads configs from configs/*.yaml and trains a causal LM with PEFT.\n","Designed for Colab T4 (8GB) usage — conservative defaults included.\n","\n","Usage:\n","    python scripts/train_base.py \\\n","        --config_dir configs \\\n","        --train_file data/train.jsonl \\\n","        --valid_file data/valid.jsonl\n","\n","\"\"\"\n","\n","import argparse\n","import os\n","import logging\n","from pathlib import Path\n","import json\n","import math\n","import random\n","from typing import Dict, List\n","\n","import torch\n","import yaml\n","from datasets import load_dataset, Dataset\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForCausalLM,\n","    DataCollatorForLanguageModeling,\n","    TrainingArguments,\n","    Trainer,\n","    BitsAndBytesConfig,\n","    set_seed,\n",")\n","from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftConfig\n","\n","logger = logging.getLogger(__name__)\n","logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\n","\n","\n","def load_yaml(path: Path) -> Dict:\n","    with open(path, \"r\", encoding=\"utf-8\") as f:\n","        return yaml.safe_load(f)\n","\n","\n","def build_prompt(prompt: str, response: str, prompt_template: str) -> str:\n","    \"\"\"Format prompt+response according to prompt_template from model_config.yaml\"\"\"\n","    return prompt_template.replace(\"{prompt}\", prompt).replace(\"{response}\", response)\n","\n","\n","def read_jsonl(path: Path) -> List[Dict]:\n","    objs = []\n","    with open(path, \"r\", encoding=\"utf-8\") as f:\n","        for line in f:\n","            line = line.strip()\n","            if not line:\n","                continue\n","            objs.append(json.loads(line))\n","    return objs\n","\n","\n","def make_dataset_from_jsonl(jsonl_path: Path, tokenizer, prompt_template: str, max_length: int):\n","    \"\"\"Load JSONL where each item has 'prompt' and 'response', return HF Dataset tokenized.\"\"\"\n","    raw = read_jsonl(jsonl_path)\n","    texts = []\n","    for item in raw:\n","        prompt = item.get(\"prompt\", \"\").strip()\n","        response = item.get(\"response\", \"\").strip()\n","        text = build_prompt(prompt, response, prompt_template)\n","        texts.append({\"text\": text})\n","    # Build a Dataset from list of dicts\n","    ds = Dataset.from_list(texts)\n","\n","    def tokenize_fn(examples):\n","        out = tokenizer(\n","            examples[\"text\"],\n","            truncation=True,\n","            max_length=max_length,\n","            padding=\"max_length\",\n","        )\n","        # Labels: for causal LM we can set labels = input_ids (Trainer will shift internally)\n","        out[\"labels\"] = out[\"input_ids\"].copy()\n","        return out\n","\n","    tokenized = ds.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n","    return tokenized\n","\n","\n","def main():\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"--config_dir\", type=str, default=\"configs\", help=\"Directory containing YAML configs\")\n","    parser.add_argument(\"--train_file\", type=str, default=\"data/train.jsonl\")\n","    parser.add_argument(\"--valid_file\", type=str, default=\"data/valid.jsonl\")\n","    parser.add_argument(\"--overwrite_output_dir\", action=\"store_true\")\n","    parser.add_argument(\"--push_to_hub\", action=\"store_true\")\n","    args = parser.parse_args()\n","\n","    cfg_dir = Path(args.config_dir)\n","    training_cfg = load_yaml(cfg_dir / \"training_args.yaml\")\n","    model_cfg = load_yaml(cfg_dir / \"model_config.yaml\")\n","    eval_cfg = load_yaml(cfg_dir / \"eval_config.yaml\")\n","\n","    # Seed\n","    seed = training_cfg.get(\"seed\", 42)\n","    set_seed(seed)\n","    random.seed(seed)\n","\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    logger.info(f\"Running on device: {device}\")\n","\n","    model_name_or_path = training_cfg.get(\"model_name_or_path\", model_cfg.get(\"model_name_or_path\"))\n","    if model_name_or_path is None:\n","        raise ValueError(\"Model name/path not specified in configs\")\n","\n","    # Tokenizer\n","    tokenizer = AutoTokenizer.from_pretrained(model_cfg.get(\"tokenizer_name_or_path\", model_name_or_path))\n","    # Ensure pad token exists (some causal models don't have pad_token)\n","    if tokenizer.pad_token is None:\n","        logger.info(\"Tokenizer has no pad_token, setting pad_token = eos_token\")\n","        tokenizer.pad_token = tokenizer.eos_token\n","\n","    # Decide quantization / bitsandbytes config\n","    use_qlora = training_cfg.get(\"qlora\", {}).get(\"use_qlora\", False)\n","    use_4bit = training_cfg.get(\"qlora\", {}).get(\"use_4bit\", False) and use_qlora\n","\n","    bnb_config = None\n","    if use_4bit:\n","        # Setup BitsAndBytes config for 4-bit loading (QLoRA)\n","        bnb_config = BitsAndBytesConfig(\n","            load_in_4bit=True,\n","            bnb_4bit_quant_type=training_cfg[\"qlora\"].get(\"bnb_4bit_quant_type\", \"nf4\"),\n","            bnb_4bit_use_double_quant=training_cfg[\"qlora\"].get(\"bnb_4bit_use_double_quant\", True),\n","            bnb_4bit_compute_dtype=getattr(torch, training_cfg[\"qlora\"].get(\"bnb_4bit_compute_dtype\", \"bfloat16\")),\n","        )\n","        logger.info(f\"Using 4-bit QLoRA bitsandbytes config: {bnb_config}\")\n","\n","    # Load model (with or without 4-bit)\n","    try:\n","        if bnb_config is not None:\n","            model = AutoModelForCausalLM.from_pretrained(\n","                model_name_or_path,\n","                quantization_config=bnb_config,\n","                device_map=\"auto\",\n","                trust_remote_code=True,\n","            )\n","        else:\n","            model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"auto\")\n","    except Exception as e:\n","        logger.warning(\"Failed to load with device_map=auto or quant config, trying CPU load as fallback. Error: %s\", e)\n","        model = AutoModelForCausalLM.from_pretrained(model_name_or_path, low_cpu_mem_usage=True)\n","        model.to(device)\n","\n","    # Prepare for k-bit training if using QLoRA\n","    if use_qlora and bnb_config is not None:\n","        logger.info(\"Preparing model for k-bit training (QLoRA flow).\")\n","        model = prepare_model_for_kbit_training(model)\n","\n","    # Build LoRA config if requested\n","    use_lora = training_cfg.get(\"lora\", {}).get(\"use_lora\", True)\n","    if use_lora:\n","        lora_cfg = training_cfg.get(\"lora\", {})\n","        lora_config = LoraConfig(\n","            r=lora_cfg.get(\"r\", 8),\n","            lora_alpha=lora_cfg.get(\"lora_alpha\", 32),\n","            target_modules=lora_cfg.get(\"target_modules\", None),\n","            lora_dropout=lora_cfg.get(\"lora_dropout\", 0.1),\n","            bias=\"none\",\n","            task_type=\"CAUSAL_LM\",\n","        )\n","        model = get_peft_model(model, lora_config)\n","        logger.info(\"LoRA adapter attached to the model.\")\n","\n","    # Prepare datasets\n","    max_input_length = model_cfg.get(\"max_input_length\", 512)\n","    train_ds = make_dataset_from_jsonl(Path(args.train_file), tokenizer, model_cfg.get(\"prompt_format\", \"{prompt}{response}\"), max_input_length)\n","    valid_ds = make_dataset_from_jsonl(Path(args.valid_file), tokenizer, model_cfg.get(\"prompt_format\", \"{prompt}{response}\"), max_input_length)\n","\n","    # Data collator\n","    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, pad_to_multiple_of=8)\n","\n","    # TrainingArguments\n","    output_dir = training_cfg.get(\"output_dir\", \"models/finetuned_model\")\n","    # Map fields from training_cfg into TrainingArguments\n","    training_args = TrainingArguments(\n","        output_dir=output_dir,\n","        per_device_train_batch_size=training_cfg.get(\"per_device_train_batch_size\", 2),\n","        per_device_eval_batch_size=training_cfg.get(\"per_device_eval_batch_size\", 4),\n","        gradient_accumulation_steps=training_cfg.get(\"gradient_accumulation_steps\", 1),\n","        num_train_epochs=training_cfg.get(\"num_train_epochs\", 2),\n","        max_steps=training_cfg.get(\"max_steps\", None),\n","        learning_rate=training_cfg.get(\"learning_rate\", 5e-5),\n","        weight_decay=training_cfg.get(\"weight_decay\", 0.0),\n","        logging_steps=training_cfg.get(\"logging_steps\", 50),\n","        evaluation_strategy=training_cfg.get(\"evaluation_strategy\", \"steps\"),\n","        eval_steps=training_cfg.get(\"eval_steps\", 200),\n","        save_steps=training_cfg.get(\"save_steps\", 200),\n","        save_total_limit=training_cfg.get(\"save_total_limit\", 3),\n","        fp16=training_cfg.get(\"fp16\", True),\n","        gradient_checkpointing=training_cfg.get(\"gradient_checkpointing\", True),\n","        load_best_model_at_end=training_cfg.get(\"load_best_model_at_end\", True),\n","        metric_for_best_model=training_cfg.get(\"metric_for_best_model\", \"loss\"),\n","        push_to_hub=training_cfg.get(\"push_to_hub\", False) or args.push_to_hub,\n","        report_to=training_cfg.get(\"report_to\", \"none\"),\n","        remove_unused_columns=False,\n","        overwrite_output_dir=args.overwrite_output_dir or training_cfg.get(\"overwrite_output_dir\", False),\n","    )\n","\n","    # Create Trainer\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_ds,\n","        eval_dataset=valid_ds,\n","        data_collator=data_collator,\n","        tokenizer=tokenizer,\n","    )\n","\n","    # Train\n","    logger.info(\"***** Running training *****\")\n","    logger.info(\"  Num train examples = %d\", len(train_ds))\n","    logger.info(\"  Num valid examples = %d\", len(valid_ds))\n","    logger.info(\"  Output dir = %s\", output_dir)\n","\n","    trainer.train()\n","    logger.info(\"Training completed. Saving model...\")\n","\n","    # Save peft adapters & tokenizer properly\n","    # If using PEFT, save_pretrained will save adapter weights\n","    model.save_pretrained(output_dir)\n","    tokenizer.save_pretrained(output_dir)\n","    logger.info(\"Saved model and tokenizer to %s\", output_dir)\n","\n","    # Optionally push to hub\n","    if training_args.push_to_hub:\n","        try:\n","            logger.info(\"Pushing model to the Hub...\")\n","            trainer.push_to_hub()\n","            logger.info(\"Pushed to Hub.\")\n","        except Exception as e:\n","            logger.warning(\"Failed to push to hub: %s\", e)\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6iSHtxH1ViYx","executionInfo":{"status":"ok","timestamp":1762687192985,"user_tz":-480,"elapsed":715,"user":{"displayName":"Andy","userId":"06581432161554596484"}},"outputId":"2d3960ff-243c-4341-92d1-0be84a0e91e9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing /content/drive/MyDrive/AIAA3102/Final_Project/Scripts/train_base.py\n"]}]},{"cell_type":"code","source":["%%writefile /content/drive/MyDrive/AIAA3102/Final_Project/Scripts/train_sparse.py"],"metadata":{"id":"k7MoxC8aWXFB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%writefile /content/drive/MyDrive/AIAA3102/Final_Project/Scripts/eval_refusal.py"],"metadata":{"id":"YKX78JWZWaWR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%writefile /content/drive/MyDrive/AIAA3102/Final_Project/Scripts/upload_to_hub.py"],"metadata":{"id":"LFSh90kKWfHR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 数据集预处理\n"],"metadata":{"id":"Owhnq__3m6HP"}},{"cell_type":"code","source":["# == Colab cell: 下载并准备 HuggingFaceH4/CodeAlpaca_20K 数据集为 train/valid/unknown_test ==\n","# 运行前请确保 !pip install datasets 已执行\n","from datasets import load_dataset\n","import random, json, re, os\n","from pathlib import Path\n","\n","random.seed(42)\n","\n","# 配置\n","# dataset_name = \"HuggingFaceH4/CodeAlpaca_20k\"  # Coding Dataset\n","dataset_name = \"Amod/mental_health_counseling_conversations\" # Counseling Dataset\n","\n","# prompt_format: 你之前 configs/model_config.yaml 使用的格式示范\n","prompt_template = \"### {Prompt}\\n### \\n{Response}\"\n","\n","# 载入（仅示例：取全部或切片以节省时间）\n","print(\"Loading dataset...\")\n","ds = load_dataset(dataset_name, split=\"train\")  # 整个 20k\n","print(\"Total examples:\", len(ds))\n","\n","# 将 dataset 转换为 prompt/response\n","examples = []\n","for ex in ds:\n","    instr = ex.get(\"prompt\",\"\") or ex.get(\"Context\",\"\") or \"\"\n","    inp = ex.get(\"input\",\"\") or \"\"\n","    out = ex.get(\"completion\",\"\") or ex.get(\"Response\",\"\") or \"\"\n","\n","    # 合并 instruction + input 为 prompt（若 input 为空就无所谓）\n","    if inp and str(inp).strip():\n","        prompt = f\"{instr}\\n{inp}\"\n","    else:\n","        prompt = instr\n","\n","    # 清理空白\n","    prompt = prompt.strip()\n","    response = out.strip()\n","    if not prompt or not response:\n","        continue\n","    examples.append({\"prompt\": prompt, \"response\": response})\n","\n","print(\"Formatted examples:\", len(examples))\n","\n","# 推荐 split: train ~ 18k, valid 1k, unknown_test 1k (总量 20k)\n","N = len(examples)\n","train_n = min(1500, N - 2000)\n","valid_n = 1000\n","unknown_n = 1000\n","\n","random.shuffle(examples)\n","train_examples = examples[:train_n]\n","valid_examples = examples[train_n:train_n+valid_n]\n","pool_for_unknown = examples[train_n+valid_n:train_n+valid_n+unknown_n*3]  # pool to perturb from\n","\n","# 简单实体扰动器（把标识符 / 库名替换为伪造名字）\n","def perturb_code_text(text, n_perturb=1):\n","    \"\"\"\n","    简单策略：找出可能的标识符 / 函数名（\\b[A-Za-z_][A-Za-z0-9_]*\\b）,\n","    随机替换 n_perturb 个为 FakeLibX / fake_funcY 等，保持格式和长度合理。\n","    \"\"\"\n","    tokens = list(set(re.findall(r\"\\b[A-Za-z_][A-Za-z0-9_]*\\b\", text)))\n","    # 过滤掉很常见的英语单词（简单黑名单）\n","    blacklist = {\"def\",\"return\",\"for\",\"in\",\"if\",\"else\",\"while\",\"import\",\"from\",\"as\",\"class\",\"True\",\"False\",\"None\",\"int\",\"str\",\"float\",\"len\",\"print\"}\n","    candidates = [t for t in tokens if t not in blacklist and not t.isdigit() and len(t) > 1]\n","    if not candidates:\n","        return text  # 无标识符可扰动\n","    perturbed = text\n","    to_replace = random.sample(candidates, min(n_perturb, len(candidates)))\n","    for i,orig in enumerate(to_replace):\n","        fake = f\"FakeLib{random.randint(100,999)}\" if orig[0].isupper() else f\"fake_fn_{random.randint(1000,9999)}\"\n","        # 使用 word boundary 替换\n","        perturbed = re.sub(rf\"\\b{re.escape(orig)}\\b\", fake, perturbed)\n","    return perturbed\n","\n","# 生成 unknown_test：对 pool 中样本做扰动，保持 prompt/response 结构，response 置空或保留（这里置空以表示未知）\n","unknown_examples = []\n","for i,ex in enumerate(pool_for_unknown[:unknown_n]):\n","    p = ex[\"prompt\"]\n","    r = ex[\"response\"]\n","    # 扰动 prompt（使其出现伪实体/伪库）\n","    p_pert = perturb_code_text(p, n_perturb=2)\n","    # 选两种 unknown 策略中的一种：把 response 设为空（模型应拒答），或保留原 response 但实体已扰动（更难）\n","    if random.random() < 0.5:\n","        unknown_examples.append({\"prompt\": p_pert, \"response\": \"\"})\n","    else:\n","        # 将 response 同步扰动（模拟：用户问伪库的问题，原回答可能不适用）\n","        r_pert = perturb_code_text(r, n_perturb=1)\n","        unknown_examples.append({\"prompt\": p_pert, \"response\": \"\"})  # 判定为 unknown，response 留空仍可\n","print(\"Unknown examples prepared:\", len(unknown_examples))\n","\n","# 写 jsonl 文件\n","def write_jsonl(lst, path):\n","    with open(path, \"w\", encoding=\"utf-8\") as f:\n","        for item in lst:\n","            f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n","    print(\"Wrote\", len(lst), \"to\", path)\n","\n","write_jsonl(train_examples, \"/content/drive/MyDrive/AIAA3102/Final_Project/Data/con_train.jsonl\")\n","write_jsonl(valid_examples, \"/content/drive/MyDrive/AIAA3102/Final_Project/Data/con_valid.jsonl\")\n","write_jsonl(unknown_examples, \"/content/drive/MyDrive/AIAA3102/Final_Project/Data/con_unknown_test.jsonl\")\n","\n","print(\"Done. Files at:\", out_dir.resolve())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":408,"referenced_widgets":["c10afdf178e34444b2ee402c0b23e422","75fbcb56d4b3411483568e3b7b8d3aad","6a5eb438562f48eb8599a75566950eee","08af865234ca475fb55f1cbcd84534f2","cd6e071fee1349889e8a9514171bcf22","016d202a7eca43a4b61743011487f3a0","a603354272b74921ad5a20027240c9fa","ed16937f3a794bb6a3b1edd57d80322f","23129249835e4a1dbca43caaf25e3a68","48f9f237ed494c9683046547a997827b","ba675e0f023646058c3178064d1aab7f","9936b94b8b6c41caa0c9f146c30feec5","2c1d9ff368ba4839a491db4af97bdd51","5c31b9f5ff0543c28739f2ca275c9a9c","353d69fd119e4a2aad5ca229006444a1","60b42b8ae946401f9f427d5e458b1e9e","5e270a72912c434dab1e44b59b70f17c","62e9caceacf14ace8bc7eb28b2c24e22","2c0b5672db3740558e2962110dcd7b50","043f5763ae5e4f3a83b287604e0204ec","02e18e07845c485e9573c93c047b0abc","3bcba4f9921a4667b885f60a5d9739a5","51226f6f18d04ee2950e89d5cc304adb","26af474b96334a1e90cadd8a5e7516ae","c65ced7fe14f4483be84ea50f8049677","577087110d73443ebfbdca4c65c0a8e3","b264bfcd4d784d39ad7e1e53ad8e8abd","7c1f5188048c45fabf02e8e98b2dad65","14e460ae33a34339adcada826341f058","ffa1a55ec4d24a03805bf9f1e7fbb647","cf43655276084c189402ef400c795bfa","53ac9eb665ba49f9a597b728fef8d557","92b29a4b918d472cb42608e786541e56"]},"id":"3VwiTNY8nCV4","executionInfo":{"status":"error","timestamp":1763958847902,"user_tz":-480,"elapsed":5795,"user":{"displayName":"Andy","userId":"06581432161554596484"}},"outputId":"bb0dd0c3-7d0f-4eb5-c80e-0e3054ef3668"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading dataset...\n"]},{"output_type":"display_data","data":{"text/plain":["README.md: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c10afdf178e34444b2ee402c0b23e422"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["combined_dataset.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9936b94b8b6c41caa0c9f146c30feec5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split:   0%|          | 0/3512 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51226f6f18d04ee2950e89d5cc304adb"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Total examples: 3512\n","Formatted examples: 3508\n","Unknown examples prepared: 1000\n","Wrote 1500 to /content/drive/MyDrive/AIAA3102/Final_Project/Data/con_train.jsonl\n","Wrote 1000 to /content/drive/MyDrive/AIAA3102/Final_Project/Data/con_valid.jsonl\n","Wrote 1000 to /content/drive/MyDrive/AIAA3102/Final_Project/Data/con_unknown_test.jsonl\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'out_dir' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1926077123.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0mwrite_jsonl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munknown_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/AIAA3102/Final_Project/Data/con_unknown_test.jsonl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done. Files at:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'out_dir' is not defined"]}]}]}